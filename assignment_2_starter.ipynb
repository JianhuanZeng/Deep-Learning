{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1M2ciAUtFYYS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cengjianhuan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a log directory\n",
    "logdir = os.mkdir(\"/Users/cengjianhuan/Downloads/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "skQBAMSgFgJF"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Dataset will be cached locally after you run this code\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# These types are required for the operation we use to compute\n",
    "# loss. Omit, and you shall receive a cryptic error message.\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "buffer_size = 5000\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oi0611F5GFyK"
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "      # FIX ME\n",
    "      # You will need to modify this function, of course.\n",
    "      # Best bet, use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "      # though if you're interested, you can write your own.\n",
    "      return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2vaut5jVGuwP"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    # You shoud not need to modify this function\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    batch_size = int(logits.shape[0])\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wxM3LBNRGLdS"
   },
   "outputs": [],
   "source": [
    "def train(model, images, labels):\n",
    "    # You should not need to modify this function\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss_value = loss(logits, labels)  \n",
    "    grads = tape.gradient(loss_value, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables))\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each experiment\n",
    "def iexperiment(model,logdir_i,epochs):    \n",
    "    step_counter = 0\n",
    "    writer = tf.contrib.summary.create_file_writer(logdir=logdir_i, flush_millis=1000)\n",
    "    with writer.as_default():\n",
    "        with tf.contrib.summary.always_record_summaries():\n",
    "            for epoch_n in range(epochs):\n",
    "                print('Epoch #%d' % (epoch_n))\n",
    "                for (batch, (images, labels)) in enumerate(train_dataset):\n",
    "                    loss_value = train(model, images, labels)\n",
    "                    tf.contrib.summary.scalar(\"loss\", loss_value, step=step_counter)\n",
    "                    step_counter +=1\n",
    "                    test_accuracy = compute_accuracy(model(x_test), y_test)\n",
    "                print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
    "                print('Accuracy #%.2f\\n' % (test_accuracy))    \n",
    "    return loss_value, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a small deep model for compared experiments\n",
    "class b_Deep_Model(tf.keras.Model):\n",
    "    def __init__(self, act, kinit):\n",
    "        super(b_Deep_Model, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(156, activation=act, kernel_initializer=kinit)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "        self.dense2 = tf.keras.layers.Dense(10,activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: The linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "acHRNledFwXL"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # self.conv = tf.keras.layers.Conv2D(filters=64,kernel_size=(5,5),padding='same',activation='elu')\n",
    "        # self.maxpooling = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        # self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "        self.dense = tf.keras.layers.Dense(10,activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # x = self.conv(x)\n",
    "        # x = self.maxpooling(x)\n",
    "        # x = self.dropout(x)  \n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1547.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19181.0,
     "status": "ok",
     "timestamp": 1.539484933147E12,
     "user": {
      "displayName": "Jianhuan Zeng",
      "photoUrl": "",
      "userId": "03066657233006930088"
     },
     "user_tz": 240.0
    },
    "id": "FncXV6IzG-In",
    "outputId": "6a9465ee-637a-4441-b9dc-cdea731cd15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #100\tLoss: 2.0517\n",
      "Step #200\tLoss: 1.8722\n",
      "Step #300\tLoss: 1.8326\n",
      "Step #400\tLoss: 1.8214\n",
      "Step #500\tLoss: 1.7275\n",
      "Step #600\tLoss: 1.6722\n",
      "Accuracy #0.81\n",
      "\n",
      "Epoch #1\n",
      "Step #700\tLoss: 1.7354\n",
      "Step #800\tLoss: 1.7313\n",
      "Step #900\tLoss: 1.7221\n",
      "Step #1000\tLoss: 1.6316\n",
      "Step #1100\tLoss: 1.6479\n",
      "Step #1200\tLoss: 1.6504\n",
      "Accuracy #0.87\n",
      "\n",
      "Epoch #2\n",
      "Step #1300\tLoss: 1.6884\n",
      "Step #1400\tLoss: 1.6326\n",
      "Step #1500\tLoss: 1.6777\n",
      "Step #1600\tLoss: 1.6363\n",
      "Step #1700\tLoss: 1.6351\n",
      "Step #1800\tLoss: 1.6023\n",
      "Accuracy #0.89\n",
      "\n",
      "Epoch #3\n",
      "Step #1900\tLoss: 1.6091\n",
      "Step #2000\tLoss: 1.6076\n",
      "Step #2100\tLoss: 1.6211\n",
      "Step #2200\tLoss: 1.6088\n",
      "Step #2300\tLoss: 1.5884\n",
      "Step #2400\tLoss: 1.6025\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #4\n",
      "Step #2500\tLoss: 1.6333\n",
      "Step #2600\tLoss: 1.6056\n",
      "Step #2700\tLoss: 1.6471\n",
      "Step #2800\tLoss: 1.6553\n",
      "Step #2900\tLoss: 1.6510\n",
      "Step #3000\tLoss: 1.6057\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #5\n",
      "Step #3100\tLoss: 1.5890\n",
      "Step #3200\tLoss: 1.6010\n",
      "Step #3300\tLoss: 1.6476\n",
      "Step #3400\tLoss: 1.5581\n",
      "Step #3500\tLoss: 1.6137\n",
      "Step #3600\tLoss: 1.5492\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #6\n",
      "Step #3700\tLoss: 1.6079\n",
      "Step #3800\tLoss: 1.5743\n",
      "Step #3900\tLoss: 1.6191\n",
      "Step #4000\tLoss: 1.5870\n",
      "Step #4100\tLoss: 1.5826\n",
      "Step #4200\tLoss: 1.5688\n",
      "Accuracy #0.91\n",
      "\n",
      "Epoch #7\n",
      "Step #4300\tLoss: 1.6140\n",
      "Step #4400\tLoss: 1.5713\n",
      "Step #4500\tLoss: 1.5951\n",
      "Step #4600\tLoss: 1.5973\n",
      "Step #4700\tLoss: 1.6084\n",
      "Step #4800\tLoss: 1.5591\n",
      "Accuracy #0.91\n",
      "\n",
      "Epoch #8\n",
      "Step #4900\tLoss: 1.5847\n",
      "Step #5000\tLoss: 1.5367\n",
      "Step #5100\tLoss: 1.5789\n",
      "Step #5200\tLoss: 1.5739\n",
      "Step #5300\tLoss: 1.5920\n",
      "Step #5400\tLoss: 1.5421\n",
      "Accuracy #0.91\n",
      "\n",
      "Epoch #9\n",
      "Step #5500\tLoss: 1.5778\n",
      "Step #5600\tLoss: 1.6050\n",
      "Step #5700\tLoss: 1.6051\n",
      "Step #5800\tLoss: 1.5486\n",
      "Step #5900\tLoss: 1.5922\n",
      "Step #6000\tLoss: 1.5568\n",
      "Accuracy #0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first time you run the below block it will crash\n",
    "# with an error 'ValueError: No variables provided.''\n",
    "# This is because the call method of your model\n",
    "# is not using any trainable variables.\n",
    "# (As written, the model just flattens the images.)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "model = Model()\n",
    "\n",
    "epochs = 10\n",
    "step_counter = 0\n",
    "\n",
    "for epoch_n in range(epochs):\n",
    "  print('Epoch #%d' % (epoch_n))\n",
    "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
    "    loss_value = train(model, images, labels)\n",
    "    step_counter +=1\n",
    "  \n",
    "    if step_counter % 100 == 0:\n",
    "      print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
    "      #print(step_counter, loss_value)\n",
    "\n",
    "  test_accuracy = compute_accuracy(model(x_test), y_test)\n",
    "  print('Accuracy #%.2f\\n' % (test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: visualization of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.7530\n",
      "Accuracy #0.81\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.6861\n",
      "Accuracy #0.83\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.6575\n",
      "Accuracy #0.83\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.6025\n",
      "Accuracy #0.83\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.6238\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #5\n",
      "Step #3600\tLoss: 1.6998\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #6\n",
      "Step #4200\tLoss: 1.6327\n",
      "Accuracy #0.88\n",
      "\n",
      "Epoch #7\n",
      "Step #4800\tLoss: 1.5644\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #8\n",
      "Step #5400\tLoss: 1.5788\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #9\n",
      "Step #6000\tLoss: 1.5766\n",
      "Accuracy #0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "model = Model()\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_linear\"\n",
    "loss, test_acc = iexperiment(model,logdir_i,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/loss_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: the deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K93oNb-4Ur3t"
   },
   "outputs": [],
   "source": [
    "class Deep_Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Deep_Model, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # self.conv = tf.keras.layers.Conv2D(filters=64,kernel_size=(5,5),padding='same',activation='elu')\n",
    "        # self.maxpooling = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "        self.dense2 = tf.keras.layers.Dense(10,activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # x = self.conv(x)\n",
    "        # x = self.maxpooling(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.6615\n",
      "Accuracy #0.81\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.6212\n",
      "Accuracy #0.83\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.5747\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.5681\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.5622\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #5\n",
      "Step #3600\tLoss: 1.6371\n",
      "Accuracy #0.85\n",
      "\n",
      "Epoch #6\n",
      "Step #4200\tLoss: 1.6243\n",
      "Accuracy #0.85\n",
      "\n",
      "Epoch #7\n",
      "Step #4800\tLoss: 1.5792\n",
      "Accuracy #0.85\n",
      "\n",
      "Epoch #8\n",
      "Step #5400\tLoss: 1.6100\n",
      "Accuracy #0.85\n",
      "\n",
      "Epoch #9\n",
      "Step #6000\tLoss: 1.5610\n",
      "Accuracy #0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "model = Deep_Model()\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_deep\"\n",
    "loss, test_acc = iexperiment(model,logdir_i,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/loss_deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6.a: The learning rates in the deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.5685\n",
      "Accuracy #0.92\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.4929\n",
      "Accuracy #0.94\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.5163\n",
      "Accuracy #0.94\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.4845\n",
      "Accuracy #0.95\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.4848\n",
      "Accuracy #0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = 0.5\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=r)\n",
    "kinit = 'glorot_uniform'\n",
    "act = 'relu'\n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6a_\"+str(r)\n",
    "loss, test_acc = iexperiment(model, logdir_i, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.5603\n",
      "Accuracy #0.89\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.5585\n",
      "Accuracy #0.91\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.5365\n",
      "Accuracy #0.92\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.5391\n",
      "Accuracy #0.92\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.5052\n",
      "Accuracy #0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = 0.1\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=r)\n",
    "kinit = 'glorot_uniform'\n",
    "act = 'relu'\n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6a_\"+str(r)\n",
    "loss, test_acc = iexperiment(model, logdir_i, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 2.2949\n",
      "Accuracy #0.13\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 2.2914\n",
      "Accuracy #0.18\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 2.2799\n",
      "Accuracy #0.21\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 2.2704\n",
      "Accuracy #0.23\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 2.2520\n",
      "Accuracy #0.27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = 0.001\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=r)\n",
    "kinit = 'glorot_uniform'\n",
    "act = 'relu'\n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6a_\"+str(r)\n",
    "loss, test_acc = iexperiment(model, logdir_i, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 2.3107\n",
      "Accuracy #0.14\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 2.3075\n",
      "Accuracy #0.14\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 2.3024\n",
      "Accuracy #0.14\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 2.3122\n",
      "Accuracy #0.14\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 2.2958\n",
      "Accuracy #0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = 0.00001\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=r)\n",
    "kinit = 'glorot_uniform'\n",
    "act = 'relu'\n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6a_\"+str(r)\n",
    "loss, test_acc = iexperiment(model, logdir_i, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/learning_rates.png)\n",
    "+ The learning rates in the experiments seems the higher, the better, which is not theoretically true. \n",
    "+ Rate 0.5 and 0.1 perform well with steady loss 1.50 and 1.55, respectively. \n",
    "+ But 0.1 and 0.5 are theoretically high rate.\n",
    "+ Both 0.001 and 0.00001 act like theoretically low rate. Rate 0.00001 is too low as the experiments, but 0.001 is usually the reasonable choice.\n",
    "+ Both the loss of 0.1 and 0.5 is slowing decreasing, far away from convergency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6.b: Different activation functions in the deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.6536\n",
      "Accuracy #0.75\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.5933\n",
      "Accuracy #0.90\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.5783\n",
      "Accuracy #0.91\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.5464\n",
      "Accuracy #0.92\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.5485\n",
      "Accuracy #0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "kinit = 'glorot_uniform'\n",
    "act='tanh'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6b_\"+act\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 2.0143\n",
      "Accuracy #0.56\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.9043\n",
      "Accuracy #0.58\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.8400\n",
      "Accuracy #0.64\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.8122\n",
      "Accuracy #0.66\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.8382\n",
      "Accuracy #0.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "kinit = 'glorot_uniform'\n",
    "act='sigmoid'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6b_\"+act\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/acti.png)\n",
    "+ Relu performs better than sigmoid and tanh as expected. \n",
    "+ Actually, tanh also performs good, where the gradient vanishing phenomenon does not occur for this test.\n",
    "+ All converge in similar step(2k) with loss 1.6 for both Relu and tanh, loss 1.8 for sigmoid\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6.c: Different gradient descent optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.8979\n",
      "Accuracy #0.56\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.8844\n",
      "Accuracy #0.55\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.7992\n",
      "Accuracy #0.56\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.9307\n",
      "Accuracy #0.57\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.8147\n",
      "Accuracy #0.64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "kinit = 'glorot_uniform'\n",
    "act='sigmoid'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6c_\"+str(1)\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.8358\n",
      "Accuracy #0.66\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.8484\n",
      "Accuracy #0.68\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.5784\n",
      "Accuracy #0.83\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.6518\n",
      "Accuracy #0.84\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.6160\n",
      "Accuracy #0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.7)\n",
    "kinit = 'glorot_uniform'\n",
    "act='sigmoid'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6c_\"+str(2)\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/optimizer.png)\n",
    "+ GradientDescentOptimizer performs the best, followed MomentumOptimizer and AdamOptimizer.\n",
    "+ GradientDescentOptimizer is relatively steady a bit faster in the step of 1.3k with the lowest loss 1.55 than that of MomentumOptimizer in the step of 2.0k with a lower loss 1.61. \n",
    "+ AdamOptimizer is slowly decreasing until the end(to the loss 1.85)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6.d: Different weight initialization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.7609\n",
      "Accuracy #0.66\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.7580\n",
      "Accuracy #0.73\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.6988\n",
      "Accuracy #0.76\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.6368\n",
      "Accuracy #0.76\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.6496\n",
      "Accuracy #0.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.7)\n",
    "kinit = 'Orthogonal'\n",
    "act='sigmoid'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6c_\"+kinit\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Step #600\tLoss: 1.8506\n",
      "Accuracy #0.67\n",
      "\n",
      "Epoch #1\n",
      "Step #1200\tLoss: 1.7984\n",
      "Accuracy #0.73\n",
      "\n",
      "Epoch #2\n",
      "Step #1800\tLoss: 1.6778\n",
      "Accuracy #0.76\n",
      "\n",
      "Epoch #3\n",
      "Step #2400\tLoss: 1.6503\n",
      "Accuracy #0.76\n",
      "\n",
      "Epoch #4\n",
      "Step #3000\tLoss: 1.6343\n",
      "Accuracy #0.82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.7)\n",
    "kinit = 'lecun_uniform'\n",
    "act='sigmoid'  \n",
    "\n",
    "model = b_Deep_Model(act, kinit)\n",
    "logdir_i = \"/Users/cengjianhuan/Downloads/logs/exp_6c_\"+kinit\n",
    "loss, test_acc = iexperiment(model,logdir_i,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/kernel_initializer.png)\n",
    "+\tweight initialization strategy 'glorot_uniform'performs the best while 'Orthogonal' and 'lecun_uniform' are similarly good. \n",
    "\n",
    "+\tthey are relatively steady in the similar step(about 1.5k), but the loss of 'glorot_uniform' is about 1.55 less than those of 'Orthogonal' and 'lecun_uniform'(about 1.7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_2_starter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
